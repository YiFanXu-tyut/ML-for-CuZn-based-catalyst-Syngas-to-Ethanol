import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Input, Dropout
from tensorflow.keras.optimizers import Adam
from bayes_opt import BayesianOptimization
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scikeras.wrappers import KerasRegressor 
from tensorflow.keras.regularizers import l2

df = pd.read_csv("EtOH.csv")
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_scaled = scaler.fit_transform(X)

def build_dnn(learning_rate, num_layers, num_units, activation):
    num_layers = int(num_layers)
    num_units = int(num_units)
    model = Sequential()
    model.add(Dense(num_units, input_dim=X_train.shape[1], activation=activation,
                    kernel_regularizer=l2(0.01))) 
    for _ in range(num_layers - 1):
        model.add(Dense(num_units, activation=activation, kernel_regularizer=l2(0.01)))
    model.add(Dense(1, activation='linear'))
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss='mean_squared_error')
    return model
def build_keras_regressor(learning_rate, num_layers, num_units, activation):
    return KerasRegressor(model=lambda: build_dnn(learning_rate, num_layers, num_units, activation),
                          epochs=50, batch_size=16, verbose=0)

pbounds = {
    'learning_rate': (1e-4, 1e-2),
    'num_layers': (1, 5),
    'num_units': (1, 20),
    'activation': (0, 2)
}
activation_map = {0: 'relu', 1: 'tanh', 2:'sigmoid'}

def dnn_evaluate_cv(learning_rate, num_layers, num_units, activation):
    activation = activation_map[int(activation)]
    model = build_keras_regressor(learning_rate, num_layers, num_units, activation)
    mse_scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)
    mean_mse = np.mean(mse_scores)
    return mean_mse
optimizer = BayesianOptimization(
    f=dnn_evaluate_cv,
    pbounds=pbounds,
    random_state=42,
    verbose=2
)
optimizer.maximize(init_points=20, n_iter=180)

best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_num_layers = int(best_params['num_layers'])
best_num_units = int(best_params['num_units'])
best_activation = activation_map[int(best_params['activation'])]
best_model = build_dnn(best_learning_rate, best_num_layers, best_num_units, best_activation)
best_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)

def evaluate_model(model, X, y):
    y_pred = model.predict(X)
    r2 = r2_score(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    return r2, rmse
train_r2, train_rmse = evaluate_model(best_model, X_train_scaled, y_train)
test_r2, test_rmse = evaluate_model(best_model, X_test_scaled, y_test)
print("\nModel Performance:")
print(f"Train R2: {train_r2:.4f}, Train RMSE: {train_rmse:.4f}")
print(f"Test R2: {test_r2:.4f}, Test RMSE: {test_rmse:.4f}")
